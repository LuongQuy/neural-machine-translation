{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_bahdanau.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLGF03Stnc-c",
        "outputId": "92df49ef-bcdf-44ed-a82e-7320724199f0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jan 26 08:35:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOnuwudTakKC",
        "outputId": "14605ce7-413a-40a7-d9f1-51b8ba401d25"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIiu1o2ZBHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a4361b-9f38-4564-9333-d07227b8e4a6"
      },
      "source": [
        "cd '/content/drive/MyDrive/vin/NLP/nmt_attention2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/vin/NLP/nmt_attention2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTJovZqak0H",
        "outputId": "ee5fa370-a78c-4670-cbaa-0f4420694594"
      },
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons==0.11.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/f8/d6fca180c123f2851035c4493690662ebdad0849a9059d56035434bff5c9/tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5wFjQmnayn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9e8e19-9944-407b-fb22-1ff9dcd96c18"
      },
      "source": [
        "import logging \r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_addons as tfa\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import unicodedata\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "import pickle\r\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.4.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W08GGDHhZmz"
      },
      "source": [
        "log_file = open('logs/log_bahdanau.log', 'a+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5z8vfwKcRgP"
      },
      "source": [
        "# load tokenizer\r\n",
        "with open('tokenizer/tokenizer.pickle', 'rb') as f:\r\n",
        "  data = pickle.load(f)\r\n",
        "  en_tokenizer = data['en_tokenizer']\r\n",
        "  vi_tokenizer = data['vi_tokenizer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMCTyOPsa57x"
      },
      "source": [
        "def preprocess_sentence(s):\r\n",
        "  s = s.lower()\r\n",
        "  s = s.strip()\r\n",
        "  s = '<s> ' + s + ' </s>'\r\n",
        "  return s\r\n",
        "\r\n",
        "en_data_tensor_path = 'sequences_data/en_data.pickle'\r\n",
        "vi_data_tensor_path = 'sequences_data/vi_data.pickle'\r\n",
        "\r\n",
        "with open(en_data_tensor_path, 'rb') as f:\r\n",
        "  en_data = pickle.load(f)\r\n",
        "  en_train = en_data['en_train_tensor']\r\n",
        "  en_dev = en_data['en_dev_tensor']\r\n",
        "  en_test = en_data['en_test_tensor']\r\n",
        "\r\n",
        "with open(vi_data_tensor_path, 'rb') as f:\r\n",
        "  vi_data = pickle.load(f)\r\n",
        "  vi_train = vi_data['vi_train_tensor']\r\n",
        "  vi_dev = vi_data['vi_dev_tensor']\r\n",
        "  vi_test = vi_data['vi_test_tensor']\r\n",
        "\r\n",
        "max_length_en, max_length_vi = en_train.shape[1], vi_train.shape[1]\r\n",
        "\r\n",
        "# save information\r\n",
        "with open('information/infor_bahdanau.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(\r\n",
        "        {'max_length_en': max_length_en, \r\n",
        "         'max_length_vi': max_length_vi,\r\n",
        "         'en_tokenizer': en_tokenizer,\r\n",
        "         'vi_tokenizer': vi_tokenizer,\r\n",
        "         'attention': 'bahdanau',\r\n",
        "         'en_example': en_train[0],\r\n",
        "         'vi_example': vi_train[0]\r\n",
        "        }, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    \r\n",
        "BUFFER_SIZE = 32000\r\n",
        "BATCH_SIZE = 128\r\n",
        "steps_per_epoch = len(en_train)//BATCH_SIZE\r\n",
        "embedding_dim = 256\r\n",
        "units = 1024\r\n",
        "vocab_en_size = len(en_tokenizer.word_index)+1\r\n",
        "vocab_vi_size = len(vi_tokenizer.word_index)+1\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train)).shuffle(BUFFER_SIZE)\r\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRV26xCEbBY6"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.enc_units = enc_units\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"embedding\")\r\n",
        "\r\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True,\r\n",
        "                                   recurrent_initializer='glorot_uniform',\r\n",
        "                                   name=\"lstm\")\r\n",
        "    \r\n",
        "  def call(self, x, hidden):\r\n",
        "    x = self.embedding(x)\r\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\r\n",
        "    return output, h, c\r\n",
        "\r\n",
        "  def initialize_hidden_state(self):\r\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8kh6v-EbVpw"
      },
      "source": [
        "class Decoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.dec_units = dec_units\r\n",
        "\r\n",
        "    self.attention_type = attention_type\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform')\r\n",
        "\r\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\r\n",
        "\r\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\r\n",
        "\r\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \r\n",
        "                                                              None, self.batch_sz*[max_length_en], self.attention_type)\r\n",
        "\r\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\r\n",
        "\r\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\r\n",
        "\r\n",
        "  def build_rnn_cell(self, batch_sz):\r\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,\r\n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\r\n",
        "    return rnn_cell\r\n",
        "\r\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\r\n",
        "\r\n",
        "    if(attention_type=='bahdanau'):\r\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n",
        "    else:\r\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n",
        "\r\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\r\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\r\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\r\n",
        "    return decoder_initial_state\r\n",
        "\r\n",
        "  def call(self, inputs, initial_state):\r\n",
        "    x = self.embedding(inputs)\r\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_vi-1])\r\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VttInpvGbZae"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\r\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))\r\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\r\n",
        "  loss = mask* loss\r\n",
        "  loss = tf.reduce_mean(loss)\r\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4uRyzDbbGn"
      },
      "source": [
        "encoder = Encoder(vocab_en_size, embedding_dim, units, BATCH_SIZE)\r\n",
        "decoder = Decoder(vocab_vi_size, embedding_dim, units, BATCH_SIZE, 'bahdanau')\r\n",
        "\r\n",
        "checkpoint_dir = './checkpoints/bahdanau_cp'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n",
        "                                 encoder=encoder,\r\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFdR3fh3X3px",
        "outputId": "b0f98dc5-6e1c-47c9-8417-44475db6e0d4"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fee704df128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KobhVei2fSjl"
      },
      "source": [
        "# BasicDecoder\r\n",
        "def evaluate_sentence(sentence):\r\n",
        "  sentence = preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  inputs = [en_tokenizer.word_index[i] for i in sentence.split(' ')]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                          maxlen=max_length_en,\r\n",
        "                                                          padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "  inference_batch_size = inputs.shape[0]\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n",
        "\r\n",
        "  dec_h = enc_h\r\n",
        "  dec_c = enc_c\r\n",
        "\r\n",
        "  start_tokens = tf.fill([inference_batch_size], vi_tokenizer.word_index['<s>'])\r\n",
        "  end_token = vi_tokenizer.word_index['</s>']\r\n",
        "\r\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\r\n",
        "\r\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\r\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\r\n",
        "\r\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\r\n",
        "\r\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n",
        "\r\n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\r\n",
        "  return outputs.sample_id.numpy()\r\n",
        "\r\n",
        "def translate(sentence):\r\n",
        "  result = evaluate_sentence(sentence)\r\n",
        "  result = vi_tokenizer.sequences_to_texts(result)\r\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNF8DKj-bsMR",
        "outputId": "c0539c17-bae0-44bf-8564-ce440e2b1c59"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(inp, targ, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\r\n",
        "\r\n",
        "\r\n",
        "    dec_input = targ[ : , :-1 ]\r\n",
        "    real = targ[ : , 1: ]\r\n",
        "\r\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\r\n",
        "\r\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\r\n",
        "    pred = decoder(dec_input, decoder_initial_state)\r\n",
        "    logits = pred.rnn_output\r\n",
        "    loss = loss_function(real, logits)\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "\r\n",
        "  return loss\r\n",
        "\r\n",
        "EPOCHS = 50\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  enc_hidden = encoder.initialize_hidden_state()\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\r\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\r\n",
        "    total_loss += batch_loss\r\n",
        "\r\n",
        "    if batch % 100 == 0:\r\n",
        "      log = 'Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n",
        "                                                   batch,\r\n",
        "                                                   batch_loss.numpy())\r\n",
        "      print(log)\r\n",
        "      log_file.writelines(log+\"\\n\")\r\n",
        "\r\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
        "  \r\n",
        "  log = 'Epoch {} Loss {:.4f} '.format(epoch + 1,\r\n",
        "                                      total_loss / steps_per_epoch)\r\n",
        "  print(log)\r\n",
        "  log_file.writelines(log+'\\n')\r\n",
        "  log = 'Time taken for 1 epoch {} sec\\n'.format(time.time() - start)\r\n",
        "  print(log)\r\n",
        "  log_file.writelines(log+'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.1901\n",
            "Epoch 1 Batch 100 Loss 0.2163\n",
            "Epoch 1 Batch 200 Loss 0.1810\n",
            "Epoch 1 Batch 300 Loss 0.1926\n",
            "Epoch 1 Batch 400 Loss 0.1756\n",
            "Epoch 1 Batch 500 Loss 0.1875\n",
            "Epoch 1 Batch 600 Loss 0.2033\n",
            "Epoch 1 Batch 700 Loss 0.1907\n",
            "Epoch 1 Batch 800 Loss 0.1911\n",
            "Epoch 1 Batch 900 Loss 0.2180\n",
            "Epoch 1 Loss 0.1981 \n",
            "Time taken for 1 epoch 1020.9594633579254 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.1827\n",
            "Epoch 2 Batch 100 Loss 0.1729\n",
            "Epoch 2 Batch 200 Loss 0.1935\n",
            "Epoch 2 Batch 300 Loss 0.1808\n",
            "Epoch 2 Batch 400 Loss 0.1764\n",
            "Epoch 2 Batch 500 Loss 0.1839\n",
            "Epoch 2 Batch 600 Loss 0.1911\n",
            "Epoch 2 Batch 700 Loss 0.1951\n",
            "Epoch 2 Batch 800 Loss 0.1875\n",
            "Epoch 2 Batch 900 Loss 0.2045\n",
            "Epoch 2 Loss 0.1892 \n",
            "Time taken for 1 epoch 989.3292467594147 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1672\n",
            "Epoch 3 Batch 100 Loss 0.1920\n",
            "Epoch 3 Batch 200 Loss 0.1766\n",
            "Epoch 3 Batch 300 Loss 0.1900\n",
            "Epoch 3 Batch 400 Loss 0.1824\n",
            "Epoch 3 Batch 500 Loss 0.1689\n",
            "Epoch 3 Batch 600 Loss 0.1549\n",
            "Epoch 3 Batch 700 Loss 0.2094\n",
            "Epoch 3 Batch 800 Loss 0.1868\n",
            "Epoch 3 Batch 900 Loss 0.1757\n",
            "Epoch 3 Loss 0.1797 \n",
            "Time taken for 1 epoch 991.1681768894196 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1636\n",
            "Epoch 4 Batch 100 Loss 0.1597\n",
            "Epoch 4 Batch 200 Loss 0.1600\n",
            "Epoch 4 Batch 300 Loss 0.1764\n",
            "Epoch 4 Batch 400 Loss 0.1655\n",
            "Epoch 4 Batch 500 Loss 0.1747\n",
            "Epoch 4 Batch 600 Loss 0.1710\n",
            "Epoch 4 Batch 700 Loss 0.1602\n",
            "Epoch 4 Batch 800 Loss 0.1937\n",
            "Epoch 4 Batch 900 Loss 0.1712\n",
            "Epoch 4 Loss 0.1727 \n",
            "Time taken for 1 epoch 991.719975233078 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1504\n",
            "Epoch 5 Batch 100 Loss 0.1288\n",
            "Epoch 5 Batch 200 Loss 0.1446\n",
            "Epoch 5 Batch 300 Loss 0.1588\n",
            "Epoch 5 Batch 400 Loss 0.1539\n",
            "Epoch 5 Batch 500 Loss 0.1515\n",
            "Epoch 5 Batch 600 Loss 0.1618\n",
            "Epoch 5 Batch 700 Loss 0.1520\n",
            "Epoch 5 Batch 800 Loss 0.1629\n",
            "Epoch 5 Batch 900 Loss 0.1796\n",
            "Epoch 5 Loss 0.1640 \n",
            "Time taken for 1 epoch 991.3916144371033 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1507\n",
            "Epoch 6 Batch 100 Loss 0.1583\n",
            "Epoch 6 Batch 200 Loss 0.1331\n",
            "Epoch 6 Batch 300 Loss 0.1642\n",
            "Epoch 6 Batch 400 Loss 0.1473\n",
            "Epoch 6 Batch 500 Loss 0.1578\n",
            "Epoch 6 Batch 600 Loss 0.1805\n",
            "Epoch 6 Batch 700 Loss 0.1533\n",
            "Epoch 6 Batch 800 Loss 0.1567\n",
            "Epoch 6 Batch 900 Loss 0.1968\n",
            "Epoch 6 Loss 0.1582 \n",
            "Time taken for 1 epoch 990.1795430183411 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1380\n",
            "Epoch 7 Batch 100 Loss 0.1456\n",
            "Epoch 7 Batch 200 Loss 0.1357\n",
            "Epoch 7 Batch 300 Loss 0.1431\n",
            "Epoch 7 Batch 400 Loss 0.1530\n",
            "Epoch 7 Batch 500 Loss 0.1530\n",
            "Epoch 7 Batch 600 Loss 0.1507\n",
            "Epoch 7 Batch 700 Loss 0.1776\n",
            "Epoch 7 Batch 800 Loss 0.1734\n",
            "Epoch 7 Batch 900 Loss 0.1642\n",
            "Epoch 7 Loss 0.1529 \n",
            "Time taken for 1 epoch 986.6906764507294 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1513\n",
            "Epoch 8 Batch 100 Loss 0.1353\n",
            "Epoch 8 Batch 200 Loss 0.1358\n",
            "Epoch 8 Batch 300 Loss 0.1341\n",
            "Epoch 8 Batch 400 Loss 0.1246\n",
            "Epoch 8 Batch 500 Loss 0.1281\n",
            "Epoch 8 Batch 600 Loss 0.1450\n",
            "Epoch 8 Batch 700 Loss 0.1604\n",
            "Epoch 8 Batch 800 Loss 0.1602\n",
            "Epoch 8 Batch 900 Loss 0.1592\n",
            "Epoch 8 Loss 0.1471 \n",
            "Time taken for 1 epoch 986.2597863674164 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1397\n",
            "Epoch 9 Batch 100 Loss 0.1444\n",
            "Epoch 9 Batch 200 Loss 0.1379\n",
            "Epoch 9 Batch 300 Loss 0.1349\n",
            "Epoch 9 Batch 400 Loss 0.1245\n",
            "Epoch 9 Batch 500 Loss 0.1441\n",
            "Epoch 9 Batch 600 Loss 0.1565\n",
            "Epoch 9 Batch 700 Loss 0.1322\n",
            "Epoch 9 Batch 800 Loss 0.1679\n",
            "Epoch 9 Batch 900 Loss 0.1588\n",
            "Epoch 9 Loss 0.1447 \n",
            "Time taken for 1 epoch 988.3453245162964 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1459\n",
            "Epoch 10 Batch 100 Loss 0.1150\n",
            "Epoch 10 Batch 200 Loss 0.1405\n",
            "Epoch 10 Batch 300 Loss 0.1333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRzX0EaXfk9H"
      },
      "source": [
        "log_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyWoFa43Y7x"
      },
      "source": [
        "# Epoch 1 Batch 0 Loss 4.1542\r\n",
        "# Epoch 1 Batch 100 Loss 2.6751\r\n",
        "# Epoch 1 Batch 200 Loss 2.2813\r\n",
        "# Epoch 1 Batch 300 Loss 2.2257\r\n",
        "# Epoch 1 Batch 400 Loss 1.9947\r\n",
        "# Epoch 1 Batch 500 Loss 2.1493\r\n",
        "# Epoch 1 Batch 600 Loss 1.8154\r\n",
        "# Epoch 1 Batch 700 Loss 2.0720\r\n",
        "# Epoch 1 Batch 800 Loss 1.7620\r\n",
        "# Epoch 1 Batch 900 Loss 1.6200\r\n",
        "# Epoch 1 Loss 2.0875\r\n",
        "# Time taken for 1 epoch 669.3499715328217 sec\r\n",
        "\r\n",
        "# Epoch 2 Batch 0 Loss 1.4498\r\n",
        "# Epoch 2 Batch 100 Loss 1.4541\r\n",
        "# Epoch 2 Batch 200 Loss 1.3935\r\n",
        "# Epoch 2 Batch 300 Loss 1.4523\r\n",
        "# Epoch 2 Batch 400 Loss 1.3322\r\n",
        "# Epoch 2 Batch 500 Loss 1.2438\r\n",
        "# Epoch 2 Batch 600 Loss 1.2254\r\n",
        "# Epoch 2 Batch 700 Loss 1.2793\r\n",
        "# Epoch 2 Batch 800 Loss 1.1715\r\n",
        "# Epoch 2 Batch 900 Loss 1.1007\r\n",
        "# Epoch 2 Loss 1.3176\r\n",
        "# Time taken for 1 epoch 652.2943453788757 sec\r\n",
        "\r\n",
        "# Epoch 3 Batch 0 Loss 1.1778\r\n",
        "# Epoch 3 Batch 100 Loss 1.0570\r\n",
        "# Epoch 3 Batch 200 Loss 1.0299\r\n",
        "# Epoch 3 Batch 300 Loss 1.1160\r\n",
        "# Epoch 3 Batch 400 Loss 1.0518\r\n",
        "# Epoch 3 Batch 500 Loss 1.0607\r\n",
        "# Epoch 3 Batch 600 Loss 0.9822\r\n",
        "# Epoch 3 Batch 700 Loss 1.1073\r\n",
        "# Epoch 3 Batch 800 Loss 1.0119\r\n",
        "# Epoch 3 Batch 900 Loss 1.0381\r\n",
        "# Epoch 3 Loss 1.0407\r\n",
        "# Time taken for 1 epoch 649.4882378578186 sec\r\n",
        "\r\n",
        "# Epoch 4 Batch 0 Loss 0.9701\r\n",
        "# Epoch 4 Batch 100 Loss 0.9343\r\n",
        "# Epoch 4 Batch 200 Loss 0.9873\r\n",
        "# Epoch 4 Batch 300 Loss 0.8650\r\n",
        "# Epoch 4 Batch 400 Loss 0.9645\r\n",
        "# Epoch 4 Batch 500 Loss 0.8636\r\n",
        "# Epoch 4 Batch 600 Loss 0.8378\r\n",
        "# Epoch 4 Batch 700 Loss 0.9464\r\n",
        "# Epoch 4 Batch 800 Loss 0.9650\r\n",
        "# Epoch 4 Batch 900 Loss 0.8431\r\n",
        "# Epoch 4 Loss 0.8990\r\n",
        "# Time taken for 1 epoch 651.2808754444122 sec\r\n",
        "\r\n",
        "# Epoch 5 Batch 0 Loss 0.8022\r\n",
        "# Epoch 5 Batch 100 Loss 0.8614\r\n",
        "# Epoch 5 Batch 200 Loss 0.7625\r\n",
        "# Epoch 5 Batch 300 Loss 0.8247\r\n",
        "# Epoch 5 Batch 400 Loss 0.7678\r\n",
        "# Epoch 5 Batch 500 Loss 0.8732\r\n",
        "# Epoch 5 Batch 600 Loss 0.7645\r\n",
        "# Epoch 5 Batch 700 Loss 0.7530\r\n",
        "# Epoch 5 Batch 800 Loss 0.8380\r\n",
        "# Epoch 5 Batch 900 Loss 0.8369\r\n",
        "# Epoch 5 Loss 0.8032\r\n",
        "# Time taken for 1 epoch 650.3041110038757 sec\r\n",
        "\r\n",
        "# Epoch 6 Batch 0 Loss 0.7799\r\n",
        "# Epoch 6 Batch 100 Loss 0.7404\r\n",
        "# Epoch 6 Batch 200 Loss 0.7077\r\n",
        "# Epoch 6 Batch 300 Loss 0.7561\r\n",
        "# Epoch 6 Batch 400 Loss 0.7539\r\n",
        "# Epoch 6 Batch 500 Loss 0.7822\r\n",
        "# Epoch 6 Batch 600 Loss 0.7440\r\n",
        "# Epoch 6 Batch 700 Loss 0.7387\r\n",
        "# Epoch 6 Batch 800 Loss 0.7043\r\n",
        "# Epoch 6 Batch 900 Loss 0.7556\r\n",
        "# Epoch 6 Loss 0.7335\r\n",
        "# Time taken for 1 epoch 649.7862718105316 sec\r\n",
        "\r\n",
        "# Epoch 7 Batch 0 Loss 0.6781\r\n",
        "# Epoch 7 Batch 100 Loss 0.6947\r\n",
        "# Epoch 7 Batch 200 Loss 0.6841\r\n",
        "# Epoch 7 Batch 300 Loss 0.5924\r\n",
        "# Epoch 7 Batch 400 Loss 0.6727\r\n",
        "# Epoch 7 Batch 500 Loss 0.6376\r\n",
        "# Epoch 7 Batch 600 Loss 0.6613\r\n",
        "# Epoch 7 Batch 700 Loss 0.7771\r\n",
        "# Epoch 7 Batch 800 Loss 0.6250\r\n",
        "# Epoch 7 Batch 900 Loss 0.6642\r\n",
        "# Epoch 7 Loss 0.6727\r\n",
        "# Time taken for 1 epoch 649.1937673091888 sec\r\n",
        "\r\n",
        "# Epoch 8 Batch 0 Loss 0.6667\r\n",
        "# Epoch 8 Batch 100 Loss 0.5860\r\n",
        "# Epoch 8 Batch 200 Loss 0.6719\r\n",
        "# Epoch 8 Batch 300 Loss 0.5738\r\n",
        "# Epoch 8 Batch 400 Loss 0.6069\r\n",
        "# Epoch 8 Batch 500 Loss 0.5105\r\n",
        "# Epoch 8 Batch 600 Loss 0.6737\r\n",
        "# Epoch 8 Batch 700 Loss 0.5844\r\n",
        "# Epoch 8 Batch 800 Loss 0.6643\r\n",
        "# Epoch 8 Batch 900 Loss 0.6837\r\n",
        "# Epoch 8 Loss 0.6205\r\n",
        "# Time taken for 1 epoch 649.8456366062164 sec\r\n",
        "\r\n",
        "# Epoch 9 Batch 0 Loss 0.6434\r\n",
        "# Epoch 9 Batch 100 Loss 0.5676\r\n",
        "# Epoch 9 Batch 200 Loss 0.5875\r\n",
        "# Epoch 9 Batch 300 Loss 0.5495\r\n",
        "# Epoch 9 Batch 400 Loss 0.5069\r\n",
        "# Epoch 9 Batch 500 Loss 0.5474\r\n",
        "# Epoch 9 Batch 600 Loss 0.5379\r\n",
        "# Epoch 9 Batch 700 Loss 0.5484\r\n",
        "# Epoch 9 Batch 800 Loss 0.6733\r\n",
        "# Epoch 9 Batch 900 Loss 0.7078\r\n",
        "# Epoch 9 Loss 0.5708\r\n",
        "# Time taken for 1 epoch 651.3016917705536 sec\r\n",
        "\r\n",
        "# Epoch 10 Batch 0 Loss 0.5394\r\n",
        "# Epoch 10 Batch 100 Loss 0.5009\r\n",
        "# Epoch 10 Batch 200 Loss 0.5534\r\n",
        "# Epoch 10 Batch 300 Loss 0.5201\r\n",
        "# Epoch 10 Batch 400 Loss 0.5188\r\n",
        "# Epoch 10 Batch 500 Loss 0.5799\r\n",
        "# Epoch 10 Batch 600 Loss 0.5131\r\n",
        "# Epoch 10 Batch 700 Loss 0.4880\r\n",
        "# Epoch 10 Batch 800 Loss 0.5090\r\n",
        "# Epoch 10 Batch 900 Loss 0.5415\r\n",
        "# Epoch 10 Loss 0.5258\r\n",
        "# Time taken for 1 epoch 652.1481170654297 sec\r\n",
        "\r\n",
        "# Epoch 11 Batch 0 Loss 0.4501\r\n",
        "# Epoch 11 Batch 100 Loss 0.4481\r\n",
        "# Epoch 11 Batch 200 Loss 0.4564\r\n",
        "# Epoch 11 Batch 300 Loss 0.5208\r\n",
        "# Epoch 11 Batch 400 Loss 0.4513\r\n",
        "# Epoch 11 Batch 500 Loss 0.4383\r\n",
        "# Epoch 11 Batch 600 Loss 0.4938\r\n",
        "# Epoch 11 Batch 700 Loss 0.5278\r\n",
        "# Epoch 11 Batch 800 Loss 0.5305\r\n",
        "# Epoch 11 Batch 900 Loss 0.5004"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QXchUyxby4a"
      },
      "source": [
        "# BasicDecoder\r\n",
        "def evaluate_sentence(sentence):\r\n",
        "  sentence = preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  inputs = [en_tokenizer.word_index[i] for i in sentence.split(' ')]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                          maxlen=max_length_en,\r\n",
        "                                                          padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "  inference_batch_size = inputs.shape[0]\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n",
        "\r\n",
        "  dec_h = enc_h\r\n",
        "  dec_c = enc_c\r\n",
        "\r\n",
        "  start_tokens = tf.fill([inference_batch_size], vi_tokenizer.word_index['<start>'])\r\n",
        "  end_token = vi_tokenizer.word_index['<end>']\r\n",
        "\r\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\r\n",
        "\r\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\r\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\r\n",
        "\r\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\r\n",
        "\r\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n",
        "\r\n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\r\n",
        "  return outputs.sample_id.numpy()\r\n",
        "\r\n",
        "def translate(sentence):\r\n",
        "  result = evaluate_sentence(sentence)\r\n",
        "  print(result)\r\n",
        "  result = vi_tokenizer.sequences_to_texts(result)\r\n",
        "  print('Input: %s' % (sentence))\r\n",
        "  print('Predicted translation: {}'.format(result))\r\n",
        "  return result\r\n",
        "\r\n",
        "translate(u'i love you .')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbh6eZ_Qb04s"
      },
      "source": [
        "# BeamSearchDecoder\r\n",
        "def beam_evaluate_sentence(sentence, beam_width=3):\r\n",
        "  sentence = preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  inputs = [en_tokenizer.word_index[i] for i in sentence.split(' ')]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                          maxlen=max_length_en,\r\n",
        "                                                          padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "  inference_batch_size = inputs.shape[0]\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n",
        "\r\n",
        "  dec_h = enc_h\r\n",
        "  dec_c = enc_c\r\n",
        "\r\n",
        "  start_tokens = tf.fill([inference_batch_size], vi_tokenizer.word_index['<start>'])\r\n",
        "  end_token = vi_tokenizer.word_index['<end>']\r\n",
        "\r\n",
        "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\r\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\r\n",
        "  print(\"beam_with * [batch_size, max_length_en, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\r\n",
        "\r\n",
        "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\r\n",
        "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\r\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\r\n",
        "\r\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\r\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n",
        "\r\n",
        "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\r\n",
        "\r\n",
        "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\r\n",
        "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\r\n",
        "\r\n",
        "  return final_outputs.numpy(), beam_scores.numpy()\r\n",
        "\r\n",
        "def beam_translate(sentence):\r\n",
        "  result, beam_scores = beam_evaluate_sentence(sentence)\r\n",
        "  print(result.shape, beam_scores.shape)\r\n",
        "  for beam, score in zip(result, beam_scores):\r\n",
        "    print(beam.shape, score.shape)\r\n",
        "    output = vi_tokenizer.sequences_to_texts(beam)\r\n",
        "    output = [a[:a.index('<end>')] for a in output]\r\n",
        "    beam_score = [a.sum() for a in score]\r\n",
        "    print('Input: %s' % (sentence))\r\n",
        "    for i in range(len(output)):\r\n",
        "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\r\n",
        "\r\n",
        "beam_translate(u'i love you .')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2HzSO_Xb9WT"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\r\n",
        "  fig = plt.figure(figsize=(10,10))\r\n",
        "  ax = fig.add_subplot(1, 1, 1)\r\n",
        "  ax.matshow(attention, cmap='viridis')\r\n",
        "\r\n",
        "  fontdict = {'fontsize': 14}\r\n",
        "\r\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\r\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lx5ecfp8PgP"
      },
      "source": [
        "#BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuQTSiEx8T-R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}