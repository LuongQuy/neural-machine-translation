{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_luong_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLGF03Stnc-c",
        "outputId": "378f17a1-fee9-437b-93cb-d322d91ed6fb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 01:15:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOnuwudTakKC",
        "outputId": "e7a7e791-39f4-4a42-8b16-3ced175e0d18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdIiu1o2ZBHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b6ce6a-213d-461f-d46d-e31ea6854bed"
      },
      "source": [
        "cd '/content/drive/MyDrive/vin/NLP/nmt_attention2'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/vin/NLP/nmt_attention2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnTJovZqak0H",
        "outputId": "f8dd787d-095a-41f2-f8eb-d519fd7fd1e3"
      },
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons==0.11.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/f8/d6fca180c123f2851035c4493690662ebdad0849a9059d56035434bff5c9/tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Found existing installation: tensorflow-addons 0.8.3\n",
            "    Uninstalling tensorflow-addons-0.8.3:\n",
            "      Successfully uninstalled tensorflow-addons-0.8.3\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5wFjQmnayn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d336cf9-30a7-4c2f-87fb-4bbab2a89c1d"
      },
      "source": [
        "import logging \r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_addons as tfa\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import io\r\n",
        "import time\r\n",
        "import pickle\r\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.4.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W08GGDHhZmz"
      },
      "source": [
        "log_file = open('logs/log_luong.log', 'a+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sfIkZkYZSTN"
      },
      "source": [
        "# load tokenizer\r\n",
        "with open('tokenizer/tokenizer.pickle', 'rb') as f:\r\n",
        "  data = pickle.load(f)\r\n",
        "  en_tokenizer = data['en_tokenizer']\r\n",
        "  vi_tokenizer = data['vi_tokenizer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMCTyOPsa57x"
      },
      "source": [
        "def preprocess_sentence(s):\r\n",
        "  s = s.lower()\r\n",
        "  s = s.strip()\r\n",
        "  s = '<s> ' + s + ' </s>'\r\n",
        "  return s\r\n",
        "\r\n",
        "en_data_tensor_path = 'sequences_data/en_data.pickle'\r\n",
        "vi_data_tensor_path = 'sequences_data/vi_data.pickle'\r\n",
        "\r\n",
        "with open(en_data_tensor_path, 'rb') as f:\r\n",
        "  en_data = pickle.load(f)\r\n",
        "  en_train = en_data['en_train_tensor']\r\n",
        "  en_dev = en_data['en_dev_tensor']\r\n",
        "  en_test = en_data['en_test_tensor']\r\n",
        "\r\n",
        "with open(vi_data_tensor_path, 'rb') as f:\r\n",
        "  vi_data = pickle.load(f)\r\n",
        "  vi_train = vi_data['vi_train_tensor']\r\n",
        "  vi_dev = vi_data['vi_dev_tensor']\r\n",
        "  vi_test = vi_data['vi_test_tensor']\r\n",
        "\r\n",
        "max_length_en, max_length_vi = en_train.shape[1], vi_train.shape[1]\r\n",
        "\r\n",
        "# save information\r\n",
        "with open('information/infor_luong.pickle', 'wb') as handle:\r\n",
        "    pickle.dump(\r\n",
        "        {'max_length_en': max_length_en, \r\n",
        "         'max_length_vi': max_length_vi, \r\n",
        "         'en_tokenizer': en_tokenizer,\r\n",
        "         'vi_tokenizer': vi_tokenizer,\r\n",
        "         'attention': 'luong',\r\n",
        "         'en_example': en_train[0],\r\n",
        "         'vi_example': vi_train[0]\r\n",
        "        }, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n",
        "    \r\n",
        "BUFFER_SIZE = 32000\r\n",
        "BATCH_SIZE = 128\r\n",
        "steps_per_epoch = len(en_train)//BATCH_SIZE\r\n",
        "embedding_dim = 256\r\n",
        "units = 1024\r\n",
        "vocab_en_size = len(en_tokenizer.word_index)+1\r\n",
        "vocab_vi_size = len(vi_tokenizer.word_index)+1\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, vi_train)).shuffle(BUFFER_SIZE)\r\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AEKVbeHV19g"
      },
      "source": [
        "en_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Tq8i2XZ1bg"
      },
      "source": [
        "vi_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xo9o4SZZwZd"
      },
      "source": [
        "del en_train\r\n",
        "del vi_train\r\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRV26xCEbBY6"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.enc_units = enc_units\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"embedding\")\r\n",
        "\r\n",
        "    ##________ LSTM layer in Encoder ------- ##\r\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True,\r\n",
        "                                   recurrent_initializer='glorot_uniform',\r\n",
        "                                   name=\"lstm\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, x, hidden):\r\n",
        "    x = self.embedding(x)\r\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\r\n",
        "    return output, h, c\r\n",
        "\r\n",
        "  def initialize_hidden_state(self):\r\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8kh6v-EbVpw"
      },
      "source": [
        "class Decoder(tf.keras.Model):\r\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.batch_sz = batch_sz\r\n",
        "    self.dec_units = dec_units\r\n",
        "    self.attention_type = attention_type\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform')\r\n",
        "\r\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\r\n",
        "\r\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\r\n",
        "\r\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \r\n",
        "                                                              None, self.batch_sz*[max_length_en], self.attention_type)\r\n",
        "\r\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\r\n",
        "\r\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\r\n",
        "\r\n",
        "\r\n",
        "  def build_rnn_cell(self, batch_sz):\r\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \r\n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\r\n",
        "    return rnn_cell\r\n",
        "\r\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\r\n",
        "\r\n",
        "    if(attention_type=='bahdanau'):\r\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n",
        "    else:\r\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\r\n",
        "\r\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\r\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\r\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\r\n",
        "    return decoder_initial_state\r\n",
        "\r\n",
        "  def call(self, inputs, initial_state):\r\n",
        "    x = self.embedding(inputs)\r\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_vi-1])\r\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VttInpvGbZae"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "def loss_function(real, pred):\r\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\r\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\r\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))\r\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \r\n",
        "  loss = mask* loss\r\n",
        "  loss = tf.reduce_mean(loss)\r\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4uRyzDbbGn"
      },
      "source": [
        "encoder = Encoder(vocab_en_size, embedding_dim, units, BATCH_SIZE)\r\n",
        "decoder = Decoder(vocab_vi_size, embedding_dim, units, BATCH_SIZE, 'luong')\r\n",
        "\r\n",
        "checkpoint_dir = './checkpoints/luong_cp'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n",
        "                                 encoder=encoder,\r\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QXchUyxby4a"
      },
      "source": [
        "# BasicDecoder\r\n",
        "def evaluate_sentence(sentence):\r\n",
        "  sentence = preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  inputs = [en_tokenizer.word_index[i] for i in sentence.split(' ')]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                          maxlen=max_length_en,\r\n",
        "                                                          padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "  inference_batch_size = inputs.shape[0]\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n",
        "\r\n",
        "  dec_h = enc_h\r\n",
        "  dec_c = enc_c\r\n",
        "\r\n",
        "  start_tokens = tf.fill([inference_batch_size], vi_tokenizer.word_index['<s>'])\r\n",
        "  end_token = vi_tokenizer.word_index['</s>']\r\n",
        "\r\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\r\n",
        "\r\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\r\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\r\n",
        "\r\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\r\n",
        "\r\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n",
        "\r\n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\r\n",
        "  return outputs.sample_id.numpy()\r\n",
        "\r\n",
        "def translate(sentence):\r\n",
        "  result = evaluate_sentence(sentence)\r\n",
        "  print(result)\r\n",
        "  result = vi_tokenizer.sequences_to_texts(result)\r\n",
        "  print('Input: %s' % (sentence))\r\n",
        "  print('Predicted translation: {}'.format(result))\r\n",
        "  return result\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF7ofpJa_qfg"
      },
      "source": [
        "def translate_eval(sentence):\r\n",
        "  result = evaluate_sentence(sentence)\r\n",
        "  result = vi_tokenizer.sequences_to_texts(result)\r\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNF8DKj-bsMR",
        "outputId": "1c9ce1d7-a67e-430b-9842-9759bed59ff7"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(inp, targ, enc_hidden):\r\n",
        "  loss = 0\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\r\n",
        "\r\n",
        "\r\n",
        "    dec_input = targ[ : , :-1 ] \r\n",
        "    real = targ[ : , 1: ]       \r\n",
        "\r\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\r\n",
        "\r\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\r\n",
        "    pred = decoder(dec_input, decoder_initial_state)\r\n",
        "    logits = pred.rnn_output\r\n",
        "    loss = loss_function(real, logits)\r\n",
        "\r\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\r\n",
        "  gradients = tape.gradient(loss, variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "\r\n",
        "  return loss\r\n",
        "\r\n",
        "EPOCHS = 50\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start = time.time()\r\n",
        "\r\n",
        "  enc_hidden = encoder.initialize_hidden_state()\r\n",
        "  total_loss = 0\r\n",
        "\r\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\r\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\r\n",
        "    total_loss += batch_loss\r\n",
        "\r\n",
        "    if batch % 100 == 0:\r\n",
        "      log = 'Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n",
        "                                                   batch,\r\n",
        "                                                   batch_loss.numpy())\r\n",
        "      print(log)\r\n",
        "      # log_file.writelines(log+\"\\n\")\r\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
        "  \r\n",
        "  log = 'Epoch {} Loss {:.4f}'.format(epoch + 1,\r\n",
        "                                      total_loss / steps_per_epoch)\r\n",
        "  print(log)\r\n",
        "  log = 'Time taken for 1 epoch {} sec\\n'.format(time.time() - start)\r\n",
        "  print(log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7310\n",
            "Epoch 1 Batch 100 Loss 0.7504\n",
            "Epoch 1 Batch 200 Loss 0.8544\n",
            "Epoch 1 Batch 300 Loss 0.8565\n",
            "Epoch 1 Batch 400 Loss 0.8591\n",
            "Epoch 1 Batch 500 Loss 0.8335\n",
            "Epoch 1 Batch 600 Loss 0.8450\n",
            "Epoch 1 Batch 700 Loss 0.8828\n",
            "Epoch 1 Batch 800 Loss 0.7997\n",
            "Epoch 1 Batch 900 Loss 0.7918\n",
            "Epoch 1 Loss 0.8252\n",
            "Time taken for 1 epoch 908.1572012901306 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6463\n",
            "Epoch 2 Batch 100 Loss 0.6953\n",
            "Epoch 2 Batch 200 Loss 0.6714\n",
            "Epoch 2 Batch 300 Loss 0.7454\n",
            "Epoch 2 Batch 400 Loss 0.6402\n",
            "Epoch 2 Batch 500 Loss 0.8249\n",
            "Epoch 2 Batch 600 Loss 0.7705\n",
            "Epoch 2 Batch 700 Loss 0.7133\n",
            "Epoch 2 Batch 800 Loss 0.7321\n",
            "Epoch 2 Batch 900 Loss 0.7391\n",
            "Epoch 2 Loss 0.7402\n",
            "Time taken for 1 epoch 908.9237003326416 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6239\n",
            "Epoch 3 Batch 100 Loss 0.5773\n",
            "Epoch 3 Batch 200 Loss 0.6617\n",
            "Epoch 3 Batch 300 Loss 0.6511\n",
            "Epoch 3 Batch 400 Loss 0.7499\n",
            "Epoch 3 Batch 500 Loss 0.6969\n",
            "Epoch 3 Batch 600 Loss 0.6382\n",
            "Epoch 3 Batch 700 Loss 0.6819\n",
            "Epoch 3 Batch 800 Loss 0.7227\n",
            "Epoch 3 Batch 900 Loss 0.7056\n",
            "Epoch 3 Loss 0.6731\n",
            "Time taken for 1 epoch 906.775988817215 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6225\n",
            "Epoch 4 Batch 100 Loss 0.6663\n",
            "Epoch 4 Batch 200 Loss 0.6087\n",
            "Epoch 4 Batch 300 Loss 0.6296\n",
            "Epoch 4 Batch 400 Loss 0.5796\n",
            "Epoch 4 Batch 500 Loss 0.5739\n",
            "Epoch 4 Batch 600 Loss 0.6755\n",
            "Epoch 4 Batch 700 Loss 0.6068\n",
            "Epoch 4 Batch 800 Loss 0.6302\n",
            "Epoch 4 Batch 900 Loss 0.6476\n",
            "Epoch 4 Loss 0.6174\n",
            "Time taken for 1 epoch 908.4974029064178 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5471\n",
            "Epoch 5 Batch 100 Loss 0.5755\n",
            "Epoch 5 Batch 200 Loss 0.5595\n",
            "Epoch 5 Batch 300 Loss 0.5308\n",
            "Epoch 5 Batch 400 Loss 0.6000\n",
            "Epoch 5 Batch 500 Loss 0.6091\n",
            "Epoch 5 Batch 600 Loss 0.5902\n",
            "Epoch 5 Batch 700 Loss 0.5892\n",
            "Epoch 5 Batch 800 Loss 0.5475\n",
            "Epoch 5 Batch 900 Loss 0.5425\n",
            "Epoch 5 Loss 0.5692\n",
            "Time taken for 1 epoch 907.3983223438263 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5039\n",
            "Epoch 6 Batch 100 Loss 0.4911\n",
            "Epoch 6 Batch 200 Loss 0.5154\n",
            "Epoch 6 Batch 300 Loss 0.5236\n",
            "Epoch 6 Batch 400 Loss 0.5356\n",
            "Epoch 6 Batch 500 Loss 0.5471\n",
            "Epoch 6 Batch 600 Loss 0.5522\n",
            "Epoch 6 Batch 700 Loss 0.5540\n",
            "Epoch 6 Batch 800 Loss 0.5005\n",
            "Epoch 6 Batch 900 Loss 0.5227\n",
            "Epoch 6 Loss 0.5237\n",
            "Time taken for 1 epoch 908.4353356361389 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.5058\n",
            "Epoch 7 Batch 100 Loss 0.4490\n",
            "Epoch 7 Batch 200 Loss 0.4139\n",
            "Epoch 7 Batch 300 Loss 0.4200\n",
            "Epoch 7 Batch 400 Loss 0.4791\n",
            "Epoch 7 Batch 500 Loss 0.4915\n",
            "Epoch 7 Batch 600 Loss 0.5504\n",
            "Epoch 7 Batch 700 Loss 0.4836\n",
            "Epoch 7 Batch 800 Loss 0.5006\n",
            "Epoch 7 Batch 900 Loss 0.4375\n",
            "Epoch 7 Loss 0.4843\n",
            "Time taken for 1 epoch 908.1231217384338 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4254\n",
            "Epoch 8 Batch 100 Loss 0.4364\n",
            "Epoch 8 Batch 200 Loss 0.4081\n",
            "Epoch 8 Batch 300 Loss 0.4523\n",
            "Epoch 8 Batch 400 Loss 0.4570\n",
            "Epoch 8 Batch 500 Loss 0.3943\n",
            "Epoch 8 Batch 600 Loss 0.4281\n",
            "Epoch 8 Batch 700 Loss 0.4284\n",
            "Epoch 8 Batch 800 Loss 0.4130\n",
            "Epoch 8 Batch 900 Loss 0.4991\n",
            "Epoch 8 Loss 0.4492\n",
            "Time taken for 1 epoch 907.2648882865906 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4285\n",
            "Epoch 9 Batch 100 Loss 0.4238\n",
            "Epoch 9 Batch 200 Loss 0.3864\n",
            "Epoch 9 Batch 300 Loss 0.4220\n",
            "Epoch 9 Batch 400 Loss 0.4029\n",
            "Epoch 9 Batch 500 Loss 0.4098\n",
            "Epoch 9 Batch 600 Loss 0.4444\n",
            "Epoch 9 Batch 700 Loss 0.3974\n",
            "Epoch 9 Batch 800 Loss 0.4323\n",
            "Epoch 9 Batch 900 Loss 0.4404\n",
            "Epoch 9 Loss 0.4159\n",
            "Time taken for 1 epoch 907.1881258487701 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3866\n",
            "Epoch 10 Batch 100 Loss 0.3896\n",
            "Epoch 10 Batch 200 Loss 0.3916\n",
            "Epoch 10 Batch 300 Loss 0.3181\n",
            "Epoch 10 Batch 400 Loss 0.4042\n",
            "Epoch 10 Batch 500 Loss 0.3890\n",
            "Epoch 10 Batch 600 Loss 0.3811\n",
            "Epoch 10 Batch 700 Loss 0.3818\n",
            "Epoch 10 Batch 800 Loss 0.3659\n",
            "Epoch 10 Batch 900 Loss 0.4240\n",
            "Epoch 10 Loss 0.3863\n",
            "Time taken for 1 epoch 906.8801140785217 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3438\n",
            "Epoch 11 Batch 100 Loss 0.3402\n",
            "Epoch 11 Batch 200 Loss 0.3532\n",
            "Epoch 11 Batch 300 Loss 0.3247\n",
            "Epoch 11 Batch 400 Loss 0.3661\n",
            "Epoch 11 Batch 500 Loss 0.3090\n",
            "Epoch 11 Batch 600 Loss 0.3644\n",
            "Epoch 11 Batch 700 Loss 0.3360\n",
            "Epoch 11 Batch 800 Loss 0.4033\n",
            "Epoch 11 Batch 900 Loss 0.3369\n",
            "Epoch 11 Loss 0.3574\n",
            "Time taken for 1 epoch 907.7584228515625 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.3397\n",
            "Epoch 12 Batch 100 Loss 0.3321\n",
            "Epoch 12 Batch 200 Loss 0.3245\n",
            "Epoch 12 Batch 300 Loss 0.3238\n",
            "Epoch 12 Batch 400 Loss 0.3454\n",
            "Epoch 12 Batch 500 Loss 0.3300\n",
            "Epoch 12 Batch 600 Loss 0.3559\n",
            "Epoch 12 Batch 700 Loss 0.3377\n",
            "Epoch 12 Batch 800 Loss 0.3472\n",
            "Epoch 12 Batch 900 Loss 0.3462\n",
            "Epoch 12 Loss 0.3325\n",
            "Time taken for 1 epoch 908.2500205039978 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2970\n",
            "Epoch 13 Batch 100 Loss 0.2943\n",
            "Epoch 13 Batch 200 Loss 0.2893\n",
            "Epoch 13 Batch 300 Loss 0.2994\n",
            "Epoch 13 Batch 400 Loss 0.2862\n",
            "Epoch 13 Batch 500 Loss 0.3090\n",
            "Epoch 13 Batch 600 Loss 0.3125\n",
            "Epoch 13 Batch 700 Loss 0.3357\n",
            "Epoch 13 Batch 800 Loss 0.3263\n",
            "Epoch 13 Batch 900 Loss 0.3286\n",
            "Epoch 13 Loss 0.3115\n",
            "Time taken for 1 epoch 909.5174980163574 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2907\n",
            "Epoch 14 Batch 100 Loss 0.3284\n",
            "Epoch 14 Batch 200 Loss 0.2845\n",
            "Epoch 14 Batch 300 Loss 0.2911\n",
            "Epoch 14 Batch 400 Loss 0.3643\n",
            "Epoch 14 Batch 500 Loss 0.3246\n",
            "Epoch 14 Batch 600 Loss 0.3159\n",
            "Epoch 14 Batch 700 Loss 0.3128\n",
            "Epoch 14 Batch 800 Loss 0.2988\n",
            "Epoch 14 Batch 900 Loss 0.3111\n",
            "Epoch 14 Loss 0.3029\n",
            "Time taken for 1 epoch 907.9089877605438 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2631\n",
            "Epoch 15 Batch 100 Loss 0.2534\n",
            "Epoch 15 Batch 200 Loss 0.2706\n",
            "Epoch 15 Batch 300 Loss 0.2427\n",
            "Epoch 15 Batch 400 Loss 0.2629\n",
            "Epoch 15 Batch 500 Loss 0.2833\n",
            "Epoch 15 Batch 600 Loss 0.2335\n",
            "Epoch 15 Batch 700 Loss 0.2522\n",
            "Epoch 15 Batch 800 Loss 0.2963\n",
            "Epoch 15 Batch 900 Loss 0.3056\n",
            "Epoch 15 Loss 0.2675\n",
            "Time taken for 1 epoch 909.9130899906158 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.2587\n",
            "Epoch 16 Batch 100 Loss 0.2420\n",
            "Epoch 16 Batch 200 Loss 0.2532\n",
            "Epoch 16 Batch 300 Loss 0.2473\n",
            "Epoch 16 Batch 400 Loss 0.2186\n",
            "Epoch 16 Batch 500 Loss 0.2710\n",
            "Epoch 16 Batch 600 Loss 0.2241\n",
            "Epoch 16 Batch 700 Loss 0.2414\n",
            "Epoch 16 Batch 800 Loss 0.2583\n",
            "Epoch 16 Batch 900 Loss 0.2751\n",
            "Epoch 16 Loss 0.2520\n",
            "Time taken for 1 epoch 907.6057608127594 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2211\n",
            "Epoch 17 Batch 100 Loss 0.2109\n",
            "Epoch 17 Batch 200 Loss 0.2253\n",
            "Epoch 17 Batch 300 Loss 0.1953\n",
            "Epoch 17 Batch 400 Loss 0.2275\n",
            "Epoch 17 Batch 500 Loss 0.2469\n",
            "Epoch 17 Batch 600 Loss 0.2212\n",
            "Epoch 17 Batch 700 Loss 0.2152\n",
            "Epoch 17 Batch 800 Loss 0.2705\n",
            "Epoch 17 Batch 900 Loss 0.2625\n",
            "Epoch 17 Loss 0.2384\n",
            "Time taken for 1 epoch 909.4178409576416 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2190\n",
            "Epoch 18 Batch 100 Loss 0.2028\n",
            "Epoch 18 Batch 200 Loss 0.2269\n",
            "Epoch 18 Batch 300 Loss 0.2216\n",
            "Epoch 18 Batch 400 Loss 0.2057\n",
            "Epoch 18 Batch 500 Loss 0.2351\n",
            "Epoch 18 Batch 600 Loss 0.2494\n",
            "Epoch 18 Batch 700 Loss 0.2122\n",
            "Epoch 18 Batch 800 Loss 0.2222\n",
            "Epoch 18 Batch 900 Loss 0.2265\n",
            "Epoch 18 Loss 0.2258\n",
            "Time taken for 1 epoch 909.1316196918488 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2077\n",
            "Epoch 19 Batch 100 Loss 0.2001\n",
            "Epoch 19 Batch 200 Loss 0.2400\n",
            "Epoch 19 Batch 300 Loss 0.2252\n",
            "Epoch 19 Batch 400 Loss 0.2014\n",
            "Epoch 19 Batch 500 Loss 0.2265\n",
            "Epoch 19 Batch 600 Loss 0.2733\n",
            "Epoch 19 Batch 700 Loss 0.2163\n",
            "Epoch 19 Batch 800 Loss 0.2328\n",
            "Epoch 19 Batch 900 Loss 0.2283\n",
            "Epoch 19 Loss 0.2167\n",
            "Time taken for 1 epoch 910.0281474590302 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1904\n",
            "Epoch 20 Batch 100 Loss 0.2198\n",
            "Epoch 20 Batch 200 Loss 0.1893\n",
            "Epoch 20 Batch 300 Loss 0.1966\n",
            "Epoch 20 Batch 400 Loss 0.2052\n",
            "Epoch 20 Batch 500 Loss 0.2003\n",
            "Epoch 20 Batch 600 Loss 0.2258\n",
            "Epoch 20 Batch 700 Loss 0.2016\n",
            "Epoch 20 Batch 800 Loss 0.2400\n",
            "Epoch 20 Batch 900 Loss 0.2171\n",
            "Epoch 20 Loss 0.2052\n",
            "Time taken for 1 epoch 908.0681865215302 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1900\n",
            "Epoch 21 Batch 100 Loss 0.1524\n",
            "Epoch 21 Batch 200 Loss 0.1999\n",
            "Epoch 21 Batch 300 Loss 0.1856\n",
            "Epoch 21 Batch 400 Loss 0.1928\n",
            "Epoch 21 Batch 500 Loss 0.2050\n",
            "Epoch 21 Batch 600 Loss 0.2008\n",
            "Epoch 21 Batch 700 Loss 0.1832\n",
            "Epoch 21 Batch 800 Loss 0.1992\n",
            "Epoch 21 Batch 900 Loss 0.2160\n",
            "Epoch 21 Loss 0.1953\n",
            "Time taken for 1 epoch 909.2781002521515 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2615\n",
            "Epoch 22 Batch 100 Loss 0.2078\n",
            "Epoch 22 Batch 200 Loss 0.1947\n",
            "Epoch 22 Batch 300 Loss 0.1865\n",
            "Epoch 22 Batch 400 Loss 0.2183\n",
            "Epoch 22 Batch 500 Loss 0.1969\n",
            "Epoch 22 Batch 600 Loss 0.1862\n",
            "Epoch 22 Batch 700 Loss 0.1847\n",
            "Epoch 22 Batch 800 Loss 0.1837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykz6vSRBgenf"
      },
      "source": [
        "log_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbh6eZ_Qb04s"
      },
      "source": [
        "# BeamSearchDecoder\r\n",
        "def beam_evaluate_sentence(sentence, beam_width=3):\r\n",
        "  sentence = preprocess_sentence(sentence)\r\n",
        "\r\n",
        "  inputs = [en_tokenizer.word_index[i] for i in sentence.split(' ')]\r\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\r\n",
        "                                                          maxlen=max_length_en,\r\n",
        "                                                          padding='post')\r\n",
        "  inputs = tf.convert_to_tensor(inputs)\r\n",
        "  inference_batch_size = inputs.shape[0]\r\n",
        "  result = ''\r\n",
        "\r\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\r\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\r\n",
        "\r\n",
        "  dec_h = enc_h\r\n",
        "  dec_c = enc_c\r\n",
        "\r\n",
        "  start_tokens = tf.fill([inference_batch_size], vi_tokenizer.word_index['<s>'])\r\n",
        "  end_token = vi_tokenizer.word_index['</s>']\r\n",
        "\r\n",
        "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\r\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\r\n",
        "  print(\"beam_with * [batch_size, max_length_en, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\r\n",
        "\r\n",
        "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\r\n",
        "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\r\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\r\n",
        "\r\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\r\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\r\n",
        "\r\n",
        "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\r\n",
        " \r\n",
        "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\r\n",
        "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\r\n",
        "\r\n",
        "  return final_outputs.numpy(), beam_scores.numpy()\r\n",
        "\r\n",
        "def beam_translate(sentence):\r\n",
        "  result, beam_scores = beam_evaluate_sentence(sentence)\r\n",
        "  print(result.shape, beam_scores.shape)\r\n",
        "  for beam, score in zip(result, beam_scores):\r\n",
        "    print(beam.shape, score.shape)\r\n",
        "    output = vi_tokenizer.sequences_to_texts(beam)\r\n",
        "    output = [a[:a.index('</s>')] for a in output]\r\n",
        "    beam_score = [a.sum() for a in score]\r\n",
        "    print('Input: %s' % (sentence))\r\n",
        "    for i in range(len(output)):\r\n",
        "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\r\n",
        "\r\n",
        "beam_translate(u'i love you .')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2HzSO_Xb9WT"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\r\n",
        "  fig = plt.figure(figsize=(10,10))\r\n",
        "  ax = fig.add_subplot(1, 1, 1)\r\n",
        "  ax.matshow(attention, cmap='viridis')\r\n",
        "\r\n",
        "  fontdict = {'fontsize': 14}\r\n",
        "\r\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\r\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\r\n",
        "\r\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lx5ecfp8PgP"
      },
      "source": [
        "#BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHqdt-qU9THQ"
      },
      "source": [
        "en_test, vi_test = load_data('data/dev/tst2012.en', 'data/dev/tst2012.vi', max_length=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuQTSiEx8T-R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}